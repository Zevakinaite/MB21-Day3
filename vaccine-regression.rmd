---
title: "Covid vaccine regression"
author: ""
date: "08/02/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## Regression problem

- We will run regression and other related models for Covid-19 vaccination data

## Libiraries

- We will use the following packages

```{r}
library(tidyverse)
library(caret)
library(glmnet)
```

## Load data

We will use the following data. It is a combined dataset from three data sources we have been using. The code for processing is available at `data_prep/data_preparation.R`.

```{r}
data_vac <- read_csv("data/vaccine-data.csv.gz") 
```


## Check data

Let's have a cursory look at the data, especially check the distribution of the output variable `Booster_Doses_18Plus_Vax_Pct` Do we need conversion?

### `head()`

```{r}
data_vac %>% head()
```

### Check the distribution of the output

```{r}
data_vac %>%
  ggplot(aes(Booster_Doses_18Plus_Vax_Pct)) +
  geom_density()
```

```{r}
data_vac %>%
  pull(Booster_Doses_18Plus_Vax_Pct) %>%
  summary()
```


## Decide the variable to include as input

- There are 47 variables what are possible predictors? Especially:
  - trump_pct
  - demography: TotalPop, Men, Women, Hispanic, White, Black, Native, Asian, Pacific, VotingAgeCitizen, Income, IncomePerCap, Poverty, ChildPoverty, Professional, Service, Office, Construction, Production, Drive, Carpool, Transit, Walk, OtherTransp, WorkAtHome, MeanCommute, Employed, PrivateWork, PublicWork, SelfEmployed, FamilyWork, Unemployment
- What do you think should be included as the inputs?


```{r}
names(data_vac)

data_vac_use <- data_vac %>%
  select(Booster_Doses_18Plus_Vax_Pct, TotalPop, Hispanic, White, Black, Income, IncomePerCap, Poverty, ChildPoverty, WorkAtHome,Unemployment, pct_trump) %>%
  drop_na()
```

## Data preparation

Here we need to prepare the data, in particular:

1. Train-test split
2. Data preprocessing

Using `caret` (or something else if you like), prepare two datasets of pre-processed train/test data.

## Train-test split

```{r}
set.seed(202302)
partition <- createDataPartition(data_vac_use$Booster_Doses_18Plus_Vax_Pct, p = .7, list = F)
vac_train_c <- data_vac_use %>% slice(partition)
vac_test_c <- data_vac_use %>% slice(-partition)
```

## Preprocess

```{r}
vac_train_X <- vac_train_c %>% select(-Booster_Doses_18Plus_Vax_Pct) # the dependant variable (and any dummy variables) should not be included in the preprocessing
vac_prep <- preProcess(vac_train_X, method = c("center", "scale"))

vac_train_c_processed <- predict(vac_prep, vac_train_c)
vac_test_c_processed <- predict(vac_prep, vac_test_c)
```


## Analysis

### Linear regression

- Run linear regression 
- Evaluate the model

```{r}
vac_model_linear <- lm(Booster_Doses_18Plus_Vax_Pct ~ ., data = vac_train_c_processed)
summary(vac_model_linear)
```

```{r}
#Prediction
Y_train <- vac_train_c_processed$Booster_Doses_18Plus_Vax_Pct
Y_hat_train <- predict(vac_model_linear, newdata = vac_train_c_processed)

RMSE_train <- (Y_train - Y_hat_train)^2 %>% mean %>%sqrt()

Y_test <- vac_test_c_processed$Booster_Doses_18Plus_Vax_Pct
Y_hat_test <- predict(vac_model_linear, newdata = vac_test_c_processed)

RMSE_test <- (Y_test - Y_hat_test)^2 %>% mean %>% sqrt()
```

### Additional movel evaluations

Using the linear regression model as the baseline we attempt two things:

1. Is it possible to improve the prediction using more flexible models?
  - KNN-regression
  - Or regression model variant of models covered in classificaiton section. 
    - For example:
      - svm: svmPoly, svmRadial works both regression and classification (svmPoly may take quite long time as the number of tuning paramters are many.)
      - trees: rf
      


```{r}
set.seed(123)
  ctrl <- trainControl(method = "repeatedcv",
  number = 5,
  repeats = 3)

vac_knn <- train(Booster_Doses_18Plus_Vax_Pct ~ .,
  data = vac_train_c_processed,
  method = "knn", trControl = ctrl,
  tuneGrid = expand.grid(k = c(2:10, 15, 20, 30, 50)))

vac_knn
```

### SVM with Radial Kernel

```{r}
y_hat_train <- predict(vac_knn, newdata = vac_train_c_processed)

RMSE_train_knn <- (vac_train_c_processed$Booster_Doses_18Plus_Vax_Pct - y_hat_train)^2 %>% mean %>% sqrt

y_hat_test <- predict(vac_knn, newdata = vac_test_c_processed)

RMSE_test_knn <- (vac_test_c_processed$Booster_Doses_18Plus_Vax_Pct - Y_hat_test)^2 %>% mean %>% sqrt()
```


## LASSO and ridge regression

- Now, let's run LASSO and/or Ridge regression. 
- What do you find? 
  - Shrinkage of the coefficients

### LASSO Outcome

```{r}
#Data preparation
vac_train_X <- vac_train_c_processed %>%
  select(-Booster_Doses_18Plus_Vax_Pct) %>% as.matrix()
vac_train_Y <- vac_train_c_processed$Booster_Doses_18Plus_Vax_Pct

vac_test_X <- vac_test_c_processed %>%
  select(-Booster_Doses_18Plus_Vax_Pct) %>% as.matrix()
vac_test_Y <- vac_test_c_processed$Booster_Doses_18Plus_Vax_Pct
```

```{r}
#Lasso regression estimation
vac_model_lasso <- cv.glmnet(vac_train_X, vac_train_Y,
  alpha = 1,                                                # 1 for lasso, if alpha = 0 this runs ridge regression
  type.measure = "mse",
  family = "gaussian")

coef(vac_model_lasso) # glmnet chooses final lambda (and therefore nr of variables) based on increase in RMSE against best model
plot(vac_model_lasso)
plot(vac_model_lasso$glmnet.fit, xvar = "lambda")
```

```{r}
#Lasso RMSE
pred_train_lasso <- as.vector(predict(vac_model_lasso, vac_train_X))
RMSE_train_lasso <- (vac_train_Y - pred_train_lasso)^2 %>% mean %>% sqrt()

pred_test_lasso <- as.vector(predict(vac_model_lasso, vac_test_X))
RMSE_test_lasso <- (vac_test_Y - pred_test_lasso)^2 %>% mean %>% sqrt()
```

#### Plot with `plot_glmnet`

Shrinkage plot of `glmnet` is not informative as it won't show the variable name. Instead you can use `plot_glmnet` in `plotmo` package.

```{r}
plotmo::plot_glmnet(vac_model_lasso$glmnet.fit, xvar = "lambda")
```



### Ridge regression outcome

```{r}
#Ridge regression estimation
vac_model_ridge <- cv.glmnet(vac_train_X, vac_train_Y,
  alpha = 0,                                                # 1 for lasso, if alpha = 0 this runs ridge regression
  type.measure = "mse",
  family = "gaussian")

coef(vac_model_ridge) # glmnet chooses final lambda (and therefore nr of variables) based on increase in RMSE against best model
plot(vac_model_ridge)
plot(vac_model_ridge$glmnet.fit, xvar = "lambda")
```
```{r}
#Ridge RMSE
pred_train_ridge <- as.vector(predict(vac_model_ridge, vac_train_X))
RMSE_train_ridge <- (vac_train_Y - pred_train_ridge)^2 %>% mean %>% sqrt()

pred_test_ridge <- as.vector(predict(vac_model_ridge, vac_test_X))
RMSE_test_ridge <- (vac_test_Y - pred_test_ridge)^2 %>% mean %>% sqrt()
```

#### Plot with `plot_glmnet`

```{r}
plotmo::plot_glmnet(vac_model_ridge$glmnet.fit, xvar = "lambda")
```

### Compare coefs: lm, lasso/ridge

Compare the cefficients across the models. What do you find?

```{r}
data.frame(linear = RMSE_test, KNN = RMSE_test_knn, LASSO = RMSE_test_lasso, ridge = RMSE_test_ridge)
data.frame(linear = RMSE_train, KNN = RMSE_train_knn, LASSO = RMSE_train_lasso, ridge = RMSE_train_ridge)
```